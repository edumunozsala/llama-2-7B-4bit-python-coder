{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMxBxlvcxC+FQOplCWVth/6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Run a scripts to fine-tune a llama-2 model on generating python code"],"metadata":{"id":"YF-vtQghNHRm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pONT4V_bzpp"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","source":["## Run the fine-tuning script"],"metadata":{"id":"oBGR7K-pNVIS"}},{"cell_type":"code","source":["!python train.py --model-name NousResearch/Llama-2-7b-hf --hf_repo edumunozsala/llama-2-7b-int4-python-18k-alpaca --trained-model-name llama-2-7b-int4-python-18k-alpaca --split train --epochs 3 --bf16"],"metadata":{"id":"J-xcI7gEcJAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python --version"],"metadata":{"id":"fymTtFGJeBz2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the model trained from the hub and test it"],"metadata":{"id":"kG9PnutyNlNY"}},{"cell_type":"code","source":["from random import randrange"],"metadata":{"id":"_RlgBhltKOIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_id = \"edumunozsala/llama-2-7b-int4-python-18k-alpaca\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, torch_dtype=torch.float16,\n","                                             device_map=device_map).to(\"cuda\")\n","\n","instruction=\"Develop a Python program that prints Hello, World! whenever it is run\"\n","input=\"\"\n","\n","prompt = f\"\"\"### Instruction:\n","Use the Task below and the Input given to write the Response, which is a programming code that can solve the Task.\n","\n","### Task:\n","{instruction}\n","\n","### Input:\n","{input}\n","\n","### Response:\n","\"\"\"\n","\n","input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n","# with torch.inference_mode():\n","outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n","\n","print(f\"Prompt:\\n{prompt}\\n\")\n","print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n","\n"],"metadata":{"id":"CSe3q4gSH4cH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yJlhMmoiKrsK"},"execution_count":null,"outputs":[]}]}